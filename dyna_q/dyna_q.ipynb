{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dyna-Q**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env():\n",
    "    def __init__(self) -> None:\n",
    "        self.coordinates = {\n",
    "            'A': ((6, 1),), # agent start position\n",
    "            'W': ((3, range(3)), (range(5, 8), 3)), # wall\n",
    "            'T': ((range(3), 8), (3, range(8, 12))), # trap\n",
    "            'P': ((6, 10), (0, 11)), # portal\n",
    "            'LP': ((1, 2),), # late portal\n",
    "            'G': ((2, 9),) # goal\n",
    "        }\n",
    "        self.generate_grid()\n",
    "        self.generate_reward_map()\n",
    "\n",
    "    def generate_grid(self) -> pd.DataFrame:\n",
    "        grid = np.zeros((8, 12), dtype=np.object0)\n",
    "        for key in list(self.coordinates.keys()):\n",
    "            for values in self.coordinates[key]:\n",
    "                grid[values] = key\n",
    "        self.grid = pd.DataFrame(grid)\n",
    "\n",
    "    def activate_late_portal(self):\n",
    "        late_portal_coord = self.coordinates.get('LP')[0]\n",
    "        self.grid.loc[late_portal_coord] = 'P'\n",
    "\n",
    "    def generate_reward_map(self) -> pd.DataFrame:\n",
    "        reward_map = np.zeros((8, 12), dtype=np.float32)\n",
    "        reward_map[self.coordinates['G'][0]] = 1\n",
    "        self.reward_map = pd.DataFrame(reward_map)\n",
    "\n",
    "    def get_reward(self, coordinates: tuple = None, reverse:bool=True) -> int:\n",
    "        \"\"\"\n",
    "        Queries the reward map and returns the reward associated to the coordinates\n",
    "        @reverse: - if the coordinates are derived from the agent state, set reverse to True\n",
    "                    They have to be reversed before querying the dataframe as \n",
    "                    agent(state) = (x,y) = pd.Dataframe.loc(y,x) with (x,y) = (col, row)\n",
    "                  - if the coordinates come from env.coordinates, then set reverse to False\n",
    "                    as they are already in the (row, col) format\n",
    "        \"\"\"\n",
    "        if reverse: return self.reward_map.loc[coordinates[::-1]]\n",
    "        else: return self.reward_map.loc[coordinates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>T</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>LP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>T</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>W</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>W</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>P</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>W</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  0  1   2  3  4  5  6  7  8  9  10 11\n",
       "0  0  0   0  0  0  0  0  0  T  0  0  P\n",
       "1  0  0  LP  0  0  0  0  0  T  0  0  0\n",
       "2  0  0   0  0  0  0  0  0  T  G  0  0\n",
       "3  W  W   W  0  0  0  0  0  T  T  T  T\n",
       "4  0  0   0  0  0  0  0  0  0  0  0  0\n",
       "5  0  0   0  W  0  0  0  0  0  0  0  0\n",
       "6  0  A   0  W  0  0  0  0  0  0  P  0\n",
       "7  0  0   0  W  0  0  0  0  0  0  0  0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Env()\n",
    "env.grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,\n",
    "                 gamma: float = 0.1,  # undiscounted task\n",
    "                 step_size: float = 0.1,\n",
    "                 epsilon: float = 0.1,\n",
    "                 ) -> None:\n",
    "        self.env = Env()\n",
    "        self.gamma = gamma\n",
    "        self.step_size = step_size\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = 4\n",
    "        self.actions = list(range(self.n_actions))\n",
    "        self.last_action = -1\n",
    "        self.last_state = -1\n",
    "        self.n_states = self.env.grid.size\n",
    "        self.start_position = self.coord_to_state(self.env.coordinates.get('A')[0][::-1])\n",
    "        self.position = self.start_position\n",
    "        self.q_values = self.init_state_action_dict()\n",
    "        self.state_visits = self.init_state_dict(initial_value=0)\n",
    "        self.random_generator = np.random.RandomState(seed=17)\n",
    "        self.done = False\n",
    "        self.n_steps = []\n",
    "        self.cumulative_reward = 0\n",
    "        self.rewards = []\n",
    "        self.trap_count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.done = False\n",
    "        self.position = self.start_position\n",
    "        self.last_action = -1\n",
    "        self.last_state = -1\n",
    "\n",
    "    def coord_to_state(self, coordinates: tuple) -> int:\n",
    "        return coordinates[0]*10 + coordinates[1]\n",
    "\n",
    "    def state_to_coord(self, state: int):\n",
    "        return (int(state//10), state % 10)\n",
    "\n",
    "    def init_state_action_dict(self) -> dict:\n",
    "        output_dict = {}\n",
    "        rows, cols = self.env.grid.index, self.env.grid.columns\n",
    "        for col in cols:\n",
    "            for row in rows:\n",
    "                output_dict[self.coord_to_state((col, row))] = np.zeros(4, dtype=np.float32)\n",
    "        return output_dict\n",
    "\n",
    "    def init_state_dict(self, initial_value) -> dict:\n",
    "        output_dict = {}\n",
    "        rows, cols = self.env.grid.index, self.env.grid.columns\n",
    "        for col in cols:\n",
    "            for row in rows:\n",
    "                output_dict[self.coord_to_state((col, row))] = initial_value\n",
    "        return output_dict\n",
    "\n",
    "    def update_coord(self, coord: tuple, action: int) -> tuple:\n",
    "        \"\"\"\n",
    "        Given a state and an action, moves the agent on the grid\n",
    "        If the agent encounters a wall or the edge of the grid, the initial position is returned\n",
    "        If the agent falls into a whole ('T') or finds the goal ('G'), the episode ends\n",
    "        \"\"\"\n",
    "        assert action in [0, 1, 2, 3], f\"Invalid action {action}\"\n",
    "        x, y = coord\n",
    "        if action == 0:\n",
    "            y -= 1\n",
    "        elif action == 1:\n",
    "            x += 1\n",
    "        elif action == 2:\n",
    "            y += 1\n",
    "        elif action == 3:\n",
    "            x -= 1\n",
    "\n",
    "        # if the action moves the agent out of bounds\n",
    "        if x not in range(0, self.env.grid.shape[1]):\n",
    "            return coord\n",
    "        if y not in range(0, self.env.grid.shape[0]):\n",
    "            return coord\n",
    "\n",
    "        # /!\\ when parsing the dataframe x and y are reversed\n",
    "        # if the agent bumps into a wall\n",
    "        if self.env.grid.loc[y, x] == 'W':\n",
    "            return coord\n",
    "        # if the agent goes through the portal\n",
    "        if self.env.grid.loc[y, x] == 'P':\n",
    "            return (11, 0)\n",
    "        # if the agent encounters falls into a trao\n",
    "        if self.env.grid.loc[y, x] == 'T':\n",
    "            self.trap_count +=1\n",
    "            self.done = True\n",
    "        # if the agent finds the treasure\n",
    "        if self.env.grid.loc[y,x] == 'G':\n",
    "            self.cumulative_reward +=1\n",
    "            self.done = True\n",
    "\n",
    "        self.position == (x, y)\n",
    "        return (x, y)\n",
    "\n",
    "    def update_state(self, state, action) -> int:\n",
    "        assert action in [0, 1, 2, 3], f\"Invalid action: {action}, should be in {[i for i in range(4)]}\"\n",
    "        coord = self.state_to_coord(state)\n",
    "        updated_coord = self.update_coord(coord, action)\n",
    "        updated_state = self.coord_to_state(updated_coord)\n",
    "        self.position = updated_state\n",
    "        self.state_visits[self.position] += 1\n",
    "        return updated_state\n",
    "\n",
    "    def argmax(self, action_values) -> int:\n",
    "        \"\"\"\n",
    "        Selects the index of the highest action value\n",
    "        Breaks ties randomly\n",
    "        \"\"\"\n",
    "        return self.random_generator.choice(np.flatnonzero(action_values == np.max(action_values)))\n",
    "\n",
    "    def epsilon_greedy(self, state) -> int:\n",
    "        \"\"\"\n",
    "        Returns an action using an epsilon-greedy policy \n",
    "        w.r.t. the current action-value function\n",
    "        \"\"\"\n",
    "        # probability of epsilon of picking a random action\n",
    "        if self.random_generator.rand() < self.epsilon:\n",
    "            action = self.random_generator.choice(self.actions)\n",
    "        # picking the action greedily w.r.t state action values\n",
    "        else:\n",
    "            action_values = self.q_values[state]\n",
    "            action = self.argmax(action_values)\n",
    "        return action\n",
    "\n",
    "    def agent_start(self, state: int):\n",
    "        \"\"\"\n",
    "        Called at the start of an episode, takes the first action \n",
    "        given the initial state\n",
    "        \"\"\"\n",
    "        self.past_state = state\n",
    "        self.past_action = self.epsilon_greedy(state)\n",
    "        # take the action\n",
    "        self.update_state(state, self.past_action)\n",
    "        return self.past_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dyna_Q_Agent(Agent):\n",
    "    def __init__(self, \n",
    "                 gamma: float = 1, \n",
    "                 step_size: float = 0.1, \n",
    "                 epsilon: float = 0.1, \n",
    "                 planning_steps: int = 100) -> None:\n",
    "        super().__init__(gamma, step_size, epsilon)\n",
    "        self.planning_steps = planning_steps\n",
    "        self.model = {}  # model[state][action] = (new state, reward)\n",
    "        self.name = \"Dyna_Q\"    \n",
    "\n",
    "    def update_model(self, last_state: int, last_action: int, state: int, reward: int) -> None:\n",
    "        \"\"\"\n",
    "        Adds a new transition to the model, if the state is encountered for \n",
    "        the first time, creates a new key\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model[last_state][last_action] = (state, reward)\n",
    "            self.state_visits[last_state] += 1\n",
    "        except KeyError:\n",
    "            self.model[last_state] = {}\n",
    "            self.model[last_state][last_action] = (state, reward)\n",
    "\n",
    "    def planning_step(self) -> None:\n",
    "            \"\"\"\n",
    "            Performs planning (indirect RL)\n",
    "            \"\"\"\n",
    "            for _ in range(self.planning_steps):\n",
    "                # select a visited state\n",
    "                planning_state = self.random_generator.choice(\n",
    "                    list(self.model.keys()))\n",
    "                # select a recorded action\n",
    "                planning_action = self.random_generator.choice(\n",
    "                    list(self.model[planning_state].keys()))\n",
    "                # get the predicted next state and reward\n",
    "                next_state, reward = self.model[planning_state][planning_action]\n",
    "                # update the values in case of terminal state\n",
    "                if next_state == -1:\n",
    "                    update = self.q_values[planning_state][planning_action]\n",
    "                    update += self.step_size * (reward - update)\n",
    "                    self.q_values[planning_state][planning_action] = update\n",
    "                # update the values in case of non-terminal state\n",
    "                else:\n",
    "                    update = self.q_values[planning_state][planning_action]\n",
    "                    update += self.step_size * (reward + self.gamma \\\n",
    "                                                * np.max(self.q_values[next_state]) - update)\n",
    "                    self.q_values[planning_state][planning_action] = update\n",
    "\n",
    "    def step(self, state: int, reward: int) -> None:\n",
    "        # direct RL update\n",
    "        update = self.q_values[self.past_state][self.past_action]\n",
    "        update += self.step_size * \\\n",
    "            (reward + self.gamma * np.max(self.q_values[state]) - update)\n",
    "        self.q_values[self.past_state][self.past_action] = update\n",
    "        # model update\n",
    "        self.update_model(self.past_state, self.past_action, state, reward)\n",
    "        # planning step\n",
    "        self.planning_step()\n",
    "        # action selection using the e-greedy policy\n",
    "        action = self.epsilon_greedy(state)\n",
    "        self.update_state(state, action)\n",
    "        # before performing the action, save the current state and action\n",
    "        self.past_state = state\n",
    "        self.past_action = action\n",
    "\n",
    "        return self.past_action\n",
    "\n",
    "    def agent_end(self) -> None:\n",
    "        \"\"\"\n",
    "        Called once the agent reaches a terminal state \n",
    "        \"\"\"\n",
    "        terminal_coordinates = self.state_to_coord(self.position)\n",
    "        # the coordinates must be reversed when querying the dataframe\n",
    "        reward = self.env.get_reward(terminal_coordinates)\n",
    "        # direct RL update for a terminal state\n",
    "        update = self.q_values[self.past_state][self.past_action]\n",
    "        update += self.step_size * (reward - update)\n",
    "        self.q_values[self.past_state][self.past_action] = update\n",
    "        # model update with next_action = -1\n",
    "        self.update_model(self.past_state, self.past_action, -1, reward)\n",
    "        self.state_visits[self.past_state] += 1\n",
    "        # planning step\n",
    "        self.planning_step()\n",
    "    \n",
    "    def play_episode(self) -> None:\n",
    "        \"\"\"\n",
    "        Plays one episode (agent_start, agent_step, agent_end)\n",
    "        Records the number of step during the episode\n",
    "        \"\"\"\n",
    "        self.agent_start(self.start_position)\n",
    "        episode_steps = 1\n",
    "        while not self.done:\n",
    "            self.step(self.position, \n",
    "                      self.env.get_reward(self.state_to_coord(self.position)))\n",
    "            episode_steps+=1\n",
    "        if self.position == self.coord_to_state(self.env.coordinates.get('G')[0]):\n",
    "            print(\"Reward!\")\n",
    "            self.cumulative_reward +=1\n",
    "        self.n_steps.append(episode_steps)\n",
    "        self.agent_end()\n",
    "        self.reset()\n",
    "    \n",
    "    def fit(self, n_episode) -> None:\n",
    "        \"\"\"\n",
    "        Plays n_episode episodes\n",
    "        \"\"\"\n",
    "        self.episode_played = 0\n",
    "        for _ in tqdm(range(n_episode), position=0, leave=True):\n",
    "            if self.episode_played == 100:\n",
    "                self.env.activate_late_portal()\n",
    "            self.play_episode()\n",
    "            self.episode_played +=1\n",
    "            self.rewards.append(self.cumulative_reward)\n",
    "        \n",
    "    def state_to_matrix(self, dictionary) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert a dictionary of states (e.g. q_values and n_visits) to a \n",
    "        matrix representation matching the environment's grid representation\n",
    "        \"\"\"\n",
    "        key_val = [(self.state_to_coord(key)[::-1], np.max(values)) \\\n",
    "            for (key, values) in list(dictionary.items())]\n",
    "        matrix = pd.DataFrame(np.zeros((8,12)))\n",
    "        for key, value in key_val:\n",
    "            matrix.loc[key] = value\n",
    "        matrix.index = [str(i) for i in matrix.index]\n",
    "        matrix.columns = [str(c) for c in matrix.columns]\n",
    "        return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dyna_Q_plus_Agent(Dyna_Q_Agent):\n",
    "    \"\"\"\n",
    "    In Dyna-Q+, a bonus reward is given for actions that haven't been tried for a long time\n",
    "    as there are greater chances that the environment dynamics have changed\n",
    "    The number of transitions since the last time (state, action) was tried is given by tau(state, action)\n",
    "    The associated reward is given by: reward + kappa * sqrt(tau(state, action))\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 gamma: float = 1, \n",
    "                 step_size: float = 0.1, \n",
    "                 epsilon: float = 0.1, \n",
    "                 planning_steps: int = 100,\n",
    "                 kappa: float = 1e-3,\n",
    "                 ) -> None:\n",
    "        super().__init__(gamma, step_size, epsilon, planning_steps)\n",
    "        self.name = \"Dyna-Q+\"\n",
    "        self.kappa = kappa\n",
    "        self.tau = self.init_state_action_dict()\n",
    "        \n",
    "    def update_model(self, last_state: int, last_action: int, state: int, reward: int) -> None:\n",
    "        \"\"\"\n",
    "        Overwrite the Dyna-Q update_model function\n",
    "        Now, when we visit a state for the first time, all the action that were not selected\n",
    "        are initialized with 0, they will be updated at each time steps according to the Dyna-Q+ algorithm\n",
    "        \"\"\"\n",
    "        if last_state not in self.model:\n",
    "            self.model[last_state] = {last_action : (state, reward)}\n",
    "        for action in self.actions:\n",
    "            if action != last_action:\n",
    "                self.model[last_state][action] = (last_state, 0)\n",
    "        else:\n",
    "            self.model[last_state][last_action] = (state, reward) \n",
    "\n",
    "    def update_tau(self, state:int, action:int) -> None:\n",
    "        for key in list(self.tau.keys()):\n",
    "            self.tau[key] +=1\n",
    "        self.tau[state][action] = 0\n",
    "            \n",
    "\n",
    "    def planning_step(self) -> None:  \n",
    "        \"\"\"\n",
    "        Overwrite the Dyna-Q planning_step function\n",
    "        Performs planning (indirect RL) and adds a bonus to the transition reward\n",
    "        The bonus is given by kappa * sqrt(tau(state, action))\n",
    "        \"\"\"\n",
    "        for _ in range(self.planning_steps):\n",
    "            # select a visited state\n",
    "            planning_state = self.random_generator.choice(list(self.model.keys()))\n",
    "            # select a recorded action\n",
    "            planning_action = self.random_generator.choice(list(self.model[planning_state].keys()))\n",
    "            # get the predicted next state and reward\n",
    "            next_state, reward = self.model[planning_state][planning_action]\n",
    "            # add the bonus reward\n",
    "            reward += self.kappa * np.sqrt(self.tau[planning_state][planning_action])\n",
    "            # update the values in case of terminal state\n",
    "            if next_state == -1:\n",
    "                update = self.q_values[planning_state][planning_action]\n",
    "                update += self.step_size * (reward - update)\n",
    "                self.q_values[planning_state][planning_action] = update\n",
    "            # update the values in case of non-terminal state\n",
    "            else:\n",
    "                update = self.q_values[planning_state][planning_action]\n",
    "                update += self.step_size * (reward + self.gamma \\\n",
    "                                            * np.max(self.q_values[next_state]) - update)\n",
    "                self.q_values[planning_state][planning_action] = update\n",
    "    \n",
    "    def step(self, state: int, reward: int) -> None:\n",
    "        \"\"\"\n",
    "        Overwrite the Dyna-Q step function\n",
    "        At every step, we increment the last visit counter for every state action by 1\n",
    "        The current state action pair is reset to 0\n",
    "        \"\"\"\n",
    "        # direct RL update\n",
    "        update = self.q_values[self.past_state][self.past_action]\n",
    "        update += self.step_size * \\\n",
    "            (reward + self.gamma * np.max(self.q_values[state]) - update)\n",
    "        self.q_values[self.past_state][self.past_action] = update\n",
    "        # model update\n",
    "        self.update_model(self.past_state, self.past_action, state, reward)\n",
    "        # planning step\n",
    "        self.planning_step()\n",
    "        # action selection using the e-greedy policy\n",
    "        action = self.epsilon_greedy(state)\n",
    "        self.update_tau(state, action)\n",
    "        self.update_state(state, action)\n",
    "        # before performing the action, save the current state and action\n",
    "        self.past_state = state\n",
    "        self.past_action = action\n",
    "\n",
    "        return self.past_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(matrix, **kwargs):\n",
    "        return go.Heatmap(z=matrix.values[::-1],\n",
    "                          x=matrix.columns,\n",
    "                          y=matrix.index[::-1],\n",
    "                          colorscale='Viridis', \n",
    "                          **kwargs)\n",
    "\n",
    "def plot_bar_chart(dataframe:pd.DataFrame, attribute:'str', color:str):\n",
    "        return go.Bar(y=dataframe[attribute], \n",
    "                      marker=dict(color=dataframe[color]))\n",
    "\n",
    "def plot_agent_performances_(agent:Agent) -> None:\n",
    "    q_values = agent.state_to_matrix(agent.q_values)\n",
    "    state_visits = agent.state_to_matrix(agent.state_visits)\n",
    "\n",
    "    n_steps = pd.DataFrame(agent.n_steps, columns=['steps'])\n",
    "    n_steps['is_optimal'] = np.where(n_steps.steps == 17,'#EF553B', '#636EFA')\n",
    "\n",
    "    # create the plots\n",
    "    heatmap1 = plot_heatmap(q_values, **{'colorbar':dict(x=0.45, y=0.78, len=0.473)})\n",
    "    heatmap2 = plot_heatmap(state_visits, **{'colorbar':dict(x=1, y=0.78, len=0.473)})\n",
    "    bar_chart = plot_bar_chart(n_steps, attribute='steps', color='is_optimal')\n",
    "\n",
    "    # Create subplot figure\n",
    "    fig = make_subplots(rows=2, cols=2, shared_xaxes=False, \n",
    "                        vertical_spacing=0.13, specs=[[{}, {}],[{\"colspan\": 2}, None]],\n",
    "                        subplot_titles=(\"State value function\",\"Number of total visits\", \"Number of steps per episode\")\n",
    "                        )\n",
    "\n",
    "    # Add the heatmaps and bar chart to the subplot figure\n",
    "    fig.add_trace(heatmap1, row=1, col=1)\n",
    "    fig.add_trace(heatmap2, row=1, col=2)\n",
    "    fig.add_trace(bar_chart, row=2, col=1)\n",
    "\n",
    "    title = f\"Agent: {agent.__class__} Number of episodes: {agent.episode_played}<br>\\\n",
    "    <span style='font-size: 13px'>Model parameters: [learning rate: {agent.step_size}, epsilon: {agent.epsilon}, discount: {agent.gamma}]</span>\"\n",
    "    fig.update_layout(height=900, width=1200, title=title)\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:30<00:00,  9.82it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "colorbar": {
          "len": 0.473,
          "x": 0.45,
          "y": 0.78
         },
         "colorscale": [
          [
           0,
           "#440154"
          ],
          [
           0.1111111111111111,
           "#482878"
          ],
          [
           0.2222222222222222,
           "#3e4989"
          ],
          [
           0.3333333333333333,
           "#31688e"
          ],
          [
           0.4444444444444444,
           "#26828e"
          ],
          [
           0.5555555555555556,
           "#1f9e89"
          ],
          [
           0.6666666666666666,
           "#35b779"
          ],
          [
           0.7777777777777778,
           "#6ece58"
          ],
          [
           0.8888888888888888,
           "#b5de2b"
          ],
          [
           1,
           "#fde725"
          ]
         ],
         "type": "heatmap",
         "x": [
          "0",
          "1",
          "2",
          "3",
          "4",
          "5",
          "6",
          "7",
          "8",
          "9",
          "10",
          "11"
         ],
         "xaxis": "x",
         "y": [
          "7",
          "6",
          "5",
          "4",
          "3",
          "2",
          "1",
          "0"
         ],
         "yaxis": "y",
         "z": [
          [
           0.27510449290275574,
           0.28516414761543274,
           0.32284602522850037,
           0,
           0.5886127352714539,
           0.5702328681945801,
           0.6472588181495667,
           0.6520848870277405,
           0.7930340766906738,
           0.7931132316589355,
           0.7930956482887268,
           0.8902035355567932
          ],
          [
           0.30139273405075073,
           0.26909923553466797,
           0.3583351671695709,
           0,
           0.6073850989341736,
           0.6283236145973206,
           0.6513369679450989,
           0.676429808139801,
           0.7310069799423218,
           0.7600305676460266,
           0,
           0.8891512155532837
          ],
          [
           0.3298037648200989,
           0.30367186665534973,
           0.3966028094291687,
           0,
           0.5886514782905579,
           0.6055423021316528,
           0.6776304244995117,
           0.7057830095291138,
           0.705366313457489,
           0.8067352771759033,
           0.7062650322914124,
           0.8878278136253357
          ],
          [
           0.3634782135486603,
           0.39817848801612854,
           0.44224366545677185,
           0.49022072553634644,
           0.4683713912963867,
           0.6070067882537842,
           0.6523105502128601,
           0.7374696135520935,
           0.7725629806518555,
           0.8116822242736816,
           0.7980265617370605,
           0.8892773985862732
          ],
          [
           0,
           0,
           0,
           0.5425692200660706,
           0.4991573691368103,
           0.6101474165916443,
           0.6771742701530457,
           0.7062798142433167,
           0,
           0,
           0,
           0
          ],
          [
           0.6348865628242493,
           0.6420173645019531,
           0.6649042963981628,
           0.6005867719650269,
           0.5503381490707397,
           0.555056095123291,
           0.6617124080657959,
           0.683532178401947,
           0,
           0,
           1.0011389255523682,
           0.9113560318946838
          ],
          [
           0.6397165060043335,
           0.6841978430747986,
           0.6641730666160583,
           0.6942976713180542,
           0.659537136554718,
           0.5788978934288025,
           0.6066004037857056,
           0.6619353294372559,
           0,
           1.0150128602981567,
           0.9021068215370178,
           0.6841541528701782
          ],
          [
           0.6301210522651672,
           0.6405240893363953,
           0.6421750783920288,
           0.6559751629829407,
           0.6369490623474121,
           0.6057869791984558,
           0.6359714269638062,
           0.643311083316803,
           0,
           0.7504181861877441,
           0.8139026165008545,
           0.7351657152175903
          ]
         ]
        },
        {
         "colorbar": {
          "len": 0.473,
          "x": 1,
          "y": 0.78
         },
         "colorscale": [
          [
           0,
           "#440154"
          ],
          [
           0.1111111111111111,
           "#482878"
          ],
          [
           0.2222222222222222,
           "#3e4989"
          ],
          [
           0.3333333333333333,
           "#31688e"
          ],
          [
           0.4444444444444444,
           "#26828e"
          ],
          [
           0.5555555555555556,
           "#1f9e89"
          ],
          [
           0.6666666666666666,
           "#35b779"
          ],
          [
           0.7777777777777778,
           "#6ece58"
          ],
          [
           0.8888888888888888,
           "#b5de2b"
          ],
          [
           1,
           "#fde725"
          ]
         ],
         "type": "heatmap",
         "x": [
          "0",
          "1",
          "2",
          "3",
          "4",
          "5",
          "6",
          "7",
          "8",
          "9",
          "10",
          "11"
         ],
         "xaxis": "x2",
         "y": [
          "7",
          "6",
          "5",
          "4",
          "3",
          "2",
          "1",
          "0"
         ],
         "yaxis": "y2",
         "z": [
          [
           245,
           179,
           168,
           0,
           93,
           88,
           53,
           61,
           42,
           40,
           34,
           1
          ],
          [
           185,
           141,
           247,
           0,
           82,
           76,
           40,
           45,
           61,
           53,
           0,
           8
          ],
          [
           161,
           294,
           288,
           0,
           81,
           70,
           42,
           57,
           62,
           24,
           21,
           7
          ],
          [
           166,
           284,
           423,
           391,
           147,
           88,
           50,
           53,
           36,
           18,
           11,
           2
          ],
          [
           0,
           0,
           0,
           294,
           93,
           53,
           47,
           36,
           9,
           1,
           2,
           2
          ],
          [
           62,
           54,
           153,
           223,
           71,
           47,
           37,
           25,
           5,
           263,
           86,
           32
          ],
          [
           52,
           49,
           44,
           146,
           73,
           52,
           32,
           22,
           12,
           489,
           316,
           258
          ],
          [
           40,
           38,
           41,
           52,
           58,
           51,
           44,
           35,
           6,
           22,
           128,
           346
          ]
         ]
        },
        {
         "marker": {
          "color": [
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#EF553B",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#EF553B",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#EF553B",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#EF553B",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#EF553B",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#EF553B",
           "#636EFA",
           "#EF553B",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#EF553B",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#EF553B",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#EF553B",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA",
           "#636EFA"
          ]
         },
         "type": "bar",
         "xaxis": "x3",
         "y": [
          38,
          189,
          90,
          134,
          31,
          167,
          192,
          12,
          38,
          85,
          16,
          285,
          205,
          112,
          65,
          97,
          31,
          30,
          38,
          26,
          31,
          29,
          89,
          41,
          15,
          39,
          37,
          33,
          27,
          22,
          35,
          28,
          22,
          25,
          23,
          32,
          16,
          32,
          12,
          19,
          18,
          327,
          26,
          192,
          32,
          28,
          43,
          24,
          37,
          24,
          12,
          34,
          23,
          24,
          30,
          38,
          44,
          33,
          25,
          25,
          21,
          19,
          19,
          10,
          19,
          17,
          22,
          19,
          23,
          19,
          29,
          51,
          23,
          33,
          76,
          146,
          153,
          26,
          40,
          17,
          19,
          21,
          20,
          25,
          25,
          19,
          26,
          21,
          57,
          44,
          40,
          20,
          24,
          57,
          27,
          21,
          23,
          31,
          29,
          23,
          23,
          41,
          28,
          25,
          21,
          20,
          51,
          75,
          52,
          30,
          42,
          21,
          29,
          22,
          31,
          22,
          14,
          14,
          16,
          14,
          16,
          18,
          14,
          14,
          15,
          16,
          19,
          16,
          14,
          17,
          14,
          14,
          14,
          18,
          18,
          16,
          15,
          14,
          14,
          24,
          23,
          36,
          12,
          12,
          14,
          12,
          12,
          12,
          12,
          14,
          14,
          12,
          12,
          12,
          12,
          22,
          12,
          13,
          12,
          13,
          13,
          14,
          15,
          17,
          12,
          12,
          12,
          14,
          14,
          13,
          12,
          16,
          17,
          14,
          12,
          14,
          12,
          12,
          72,
          134,
          17,
          30,
          17,
          14,
          11,
          12,
          14,
          20,
          14,
          13,
          15,
          14,
          26,
          15,
          12,
          12,
          15,
          14,
          12,
          14,
          13,
          14,
          13,
          12,
          12,
          16,
          16,
          12,
          14,
          12,
          13,
          12,
          14,
          32,
          16,
          12,
          12,
          16,
          14,
          16,
          16,
          13,
          22,
          14,
          16,
          15,
          22,
          12,
          14,
          14,
          24,
          17,
          12,
          14,
          14,
          24,
          12,
          12,
          12,
          16,
          20,
          20,
          45,
          19,
          26,
          15,
          14,
          13,
          13,
          12,
          12,
          14,
          12,
          14,
          19,
          25,
          26,
          18,
          15,
          14,
          13,
          12,
          12,
          12,
          17,
          13,
          21,
          13,
          15,
          12,
          14,
          12,
          12,
          12,
          12,
          12,
          13,
          14,
          14,
          20,
          13,
          12,
          17,
          12,
          12,
          12,
          12,
          19,
          12,
          13,
          12,
          18,
          14,
          19,
          34,
          18,
          30,
          16,
          16,
          14
         ],
         "yaxis": "y3"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "State value function",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Number of total visits",
          "x": 0.775,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Number of steps per episode",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.435,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 900,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Agent: <class '__main__.Dyna_Q_plus_Agent'> Number of episodes: 300<br>    <span style='font-size: 13px'>Model parameters: [learning rate: 0.125, epsilon: 0.1, discount: 0.9]</span>"
        },
        "width": 1200,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.45
         ]
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.55,
          1
         ]
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.565,
          1
         ]
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.565,
          1
         ]
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0,
          0.435
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = Dyna_Q_plus_Agent(planning_steps=100, epsilon=0.1, gamma=0.9, step_size=0.125)\n",
    "a.fit(300)\n",
    "plot_agent_performances_(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
